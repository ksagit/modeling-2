{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dat import Dat\n",
    "data = Dat()\n",
    "data.add_dict('wiki')\n",
    "data.add_dict('twitter')\n",
    "data.make_gen_dict()\n",
    "X,y = data.getXy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def jd(v,w): #Jaccard metric distance between two frames\n",
    "    ci  = np.sum(v)\n",
    "    cj  = np.sum(w)\n",
    "    cij = np.sum((v + w)//2)\n",
    "    if cij == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(1 - (cij)/(ci + cj - cij))\n",
    "    \n",
    "def cs(v,w):\n",
    "    return(cosine_similarity([v], [w])[0][0])\n",
    "    \n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "n_frames = y.shape[1]\n",
    "frame_dist_matrix = np.zeros((n_frames, n_frames))\n",
    "for i in range(0, n_frames):\n",
    "    for j in range(i, n_frames):\n",
    "        d = jd(y[:,i],y[:,j])\n",
    "        frame_dist_matrix[i][j] = d\n",
    "        frame_dist_matrix[j][i] = d\n",
    "        \n",
    "n_verbs = y.shape[0]\n",
    "verb_syntax_dist_matrix = np.zeros((n_verbs, n_verbs))\n",
    "for i in range(0, n_verbs):\n",
    "    for j in range(i, n_verbs):\n",
    "        d = jd(y[i],y[j])\n",
    "        verb_syntax_dist_matrix[i][j] = d\n",
    "        verb_syntax_dist_matrix[j][i] = d\n",
    "        \n",
    "verb_cossim_dist_matrix = np.zeros((n_verbs, n_verbs))\n",
    "for i in range(0, n_verbs):\n",
    "    for j in range(i, n_verbs):\n",
    "        d = cs(X[i],X[j])\n",
    "        verb_cossim_dist_matrix[i][j] = d\n",
    "        verb_cossim_dist_matrix[j][i] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "scipy.io.savemat('fsd.mat', mdict={'frame_syntax_distance': frame_dist_matrix})\n",
    "scipy.io.savemat('vsd.mat', mdict={'verb_syntax_distance': verb_syntax_dist_matrix})\n",
    "scipy.io.savemat('vcd.mat', mdict={'verb_cossim_distance': verb_cossim_dist_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77103705785774412"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1 = np.array(scipy.io.loadmat(\"si.mat\")[\"S1\"]).T[0]\n",
    "S2 = np.array(scipy.io.loadmat(\"si.mat\")[\"S2\"]).T[0]\n",
    "S3 = np.array(scipy.io.loadmat(\"si.mat\")[\"S3\"]).T[0]\n",
    "S4 = np.array(scipy.io.loadmat(\"si.mat\")[\"S4\"]).T[0]\n",
    "S5 = np.array(scipy.io.loadmat(\"si.mat\")[\"S5\"]).T[0]\n",
    "S6 = np.array(scipy.io.loadmat(\"si.mat\")[\"S6\"]).T[0]\n",
    "S7 = np.array(scipy.io.loadmat(\"si.mat\")[\"S7\"]).T[0]\n",
    "\n",
    "consensus = np.zeros((292, 292))\n",
    "f_list = []\n",
    "for i in range(0, 291):\n",
    "    for j in range(i, 291):\n",
    "        b1 = int(S1[i] == S1[j])\n",
    "        b2 = int(S2[i] == S2[j])\n",
    "        b3 = int(S3[i] == S3[j])\n",
    "        b4 = int(S4[i] == S4[j])\n",
    "        b5 = int(S5[i] == S5[j])\n",
    "        b6 = int(S6[i] == S6[j])\n",
    "        b7 = int(S7[i] == S7[j])\n",
    "        f = (b1 + b2 + b3 + b4 + b5 + b6 + b7)/7\n",
    "        f_list += [f]\n",
    "        consensus[i][j] = f\n",
    "        consensus[j][i] = f\n",
    "\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ami\n",
    "l = [S1, S2, S3, S4, S5, S6, S7]\n",
    "ami_list = []\n",
    "for i in range(0, 7):\n",
    "    for j in range(i+1, 7):\n",
    "        ami_list += [ami(l[i], l[j])]\n",
    "        \n",
    "np.mean(ami_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determing list of diagnostic frames, \n",
    "        \n",
    "\n",
    "        \n",
    "aff = np.exp(- frame_dist_matrix ** 2 / (2. * 1 ** 2)) #Gaussian heat kernel\n",
    "\n",
    "n_diagnostic_frames = 50\n",
    "\n",
    "sc_frames = SpectralClustering(n_clusters = n_diagnostic_frames, affinity = \"precomputed\")\n",
    "frame_cluster_predictions = sc_frames.fit_predict(aff)\n",
    "frame_cluster_pairs = list(enumerate(frame_cluster_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "glove_verbs = list(map(lambda v : (v[0]), list(gen_dict.items())))\n",
    "glove_vecs = np.array(list(map(lambda v : (v[1][1]), list(gen_dict.items()))))\n",
    "\n",
    "n_diagnostic_verbs = 100\n",
    "\n",
    "kmc_verbs = KMeans(n_clusters = n_diagnostic_verbs)\n",
    "verb_cluster_predictions = kmc_verbs.fit_predict(glove_vecs)\n",
    "\n",
    "verb_cluster_pairs = list(enumerate(verb_cluster_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_sample(l, n_clusters):\n",
    "    res = []\n",
    "    random.shuffle(l)\n",
    "    for i in range(0, n_clusters):\n",
    "        for v,c in l:\n",
    "            if i == c:\n",
    "                bv = v\n",
    "        res += [bv]\n",
    "    return(res)\n",
    "        \n",
    "import pandas as pd\n",
    "\n",
    "def maketable():\n",
    "    diagnostic_verb_list = [glove_verbs[i] for i in cluster_sample(verb_cluster_pairs, n_diagnostic_verbs)]\n",
    "    diagnostic_frame_list = [frame_list[i] for i in cluster_sample(frame_cluster_pairs, n_diagnostic_frames)]\n",
    "\n",
    "    if len(diagnostic_verb_list) - len(list(set(diagnostic_verb_list))) > 0:\n",
    "        return(maketable())\n",
    "    elif len(diagnostic_frame_list) - len(list(set(diagnostic_frame_list))) > 0:\n",
    "        return(maketable())\n",
    "    else:\n",
    "        data = np.zeros((n_diagnostic_verbs, n_diagnostic_frames))\n",
    "        for i in range(0, n_diagnostic_verbs):\n",
    "            for j in range(0, n_diagnostic_frames):\n",
    "                data[i][j] = verbnet_dict[diagnostic_verb_list[i]][ frame_list.index(diagnostic_frame_list[j]) ]\n",
    "\n",
    "        df = pd.DataFrame(data, index = diagnostic_verb_list, columns = diagnostic_frame_list, dtype=int)\n",
    "\n",
    "\n",
    "        for i in range(0, n_diagnostic_verbs//2):\n",
    "            verb = pd.Series.argmin(df.sum(axis=1))\n",
    "            df = df.drop(verb, axis=0)\n",
    "\n",
    "        for i in range(0, n_diagnostic_frames//2):\n",
    "            frame = pd.Series.argmin(df.sum(axis=0))\n",
    "            df = df.drop(frame, axis=1)\n",
    "\n",
    "        return(df)\n",
    "\n",
    "btable = maketable()\n",
    "bcount = pd.DataFrame.sum(df.sum(axis=0))\n",
    "for i in range(0, 100):\n",
    "    t = maketable()\n",
    "    count = pd.DataFrame.sum(t.sum(axis=0))\n",
    "    if count > bcount:\n",
    "        btable = t\n",
    "        bcount = count\n",
    "df = btable\n",
    "\n",
    "print(\"Sparsity = \", pd.DataFrame.sum(df.sum(axis=0)) / (n_diagnostic_frames//2 * n_diagnostic_verbs//2))\n",
    "print(\"Size =\", df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
