{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored Wikipedia GLoVe dictionary.\n"
     ]
    }
   ],
   "source": [
    "wikiglove_dict = {}\n",
    "\n",
    "file = open('/Users/kylesargent/Desktop/GLoVe + VerbNet/glove.6B.300d.txt')\n",
    "for row in file:\n",
    "    key = row[:row.index(' ')]\n",
    "    value = list(map(lambda x : float(x), str.split(row[row.index(' ') + 1:])))\n",
    "    wikiglove_dict[key] = value\n",
    "    \n",
    "print(\"Stored Wikipedia GLoVe dictionary.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored Twitter GLoVe dictionary.\n"
     ]
    }
   ],
   "source": [
    "twitterglove_dict = {}\n",
    "\n",
    "file = open('/Users/kylesargent/Desktop/GLoVe + VerbNet/glove.twitter.27B.100d.txt')\n",
    "for row in file:\n",
    "    key = row[:row.index(' ')]\n",
    "    value = list(map(lambda x : float(x), str.split(row[row.index(' ') + 1:])))\n",
    "    twitterglove_dict[key] = value\n",
    "    \n",
    "print(\"Stored Twitter GLoVe dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored VerbNet dictionary.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "reader = csv.reader(open('/Users/kylesargent/Desktop/GLoVe + VerbNet/verb_frames.csv', 'r'))\n",
    "verbnet_dict = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    verbnet_dict[k] = json.loads(v)\n",
    "    \n",
    "print(\"Stored VerbNet dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_list = ['It V', 'It V NP.theme', 'It V PP.experiencer that S', 'It V PP.theme', 'It V that S', 'NP NP V ADVP-Middle together', 'NP NP V together', 'NP v', 'NP V', 'NP V', 'NP V ADJ', 'NP V ADJ PP.experiencer', 'NP V ADJ-Middle', 'NP V ADJP NP', 'NP V ADV', 'NP V ADV-Middle', 'NP V ADV-Middle PP', 'NP V ADV-Middle PP.location', 'NP V ADV-Middle together', 'NP V ADVP', 'NP V ADVP-Middle', 'NP V ADVP-Middle PP', 'NP V apart', 'NP V down NP', 'NP V for NP S_INF', 'NP V how S', 'NP V how S_INF', 'NP V NP', 'NP V NP', 'NP V NP ADJ', 'NP V NP ADJ PP.instrument', 'NP V NP ADJP', 'NP V NP ADJP PP.instrument', 'NP V NP ADJP PP.result', 'NP V NP ADJP-Result', 'NP V NP ADV', 'NP V NP ADVP', 'NP V NP apart', 'NP V NP down', 'NP V NP how S', 'NP V NP how S_INF', 'NP V NP NP', 'NP V NP NP PP.asset', 'NP V NP NP PP.theme', 'NP V NP NP together', 'NP V NP NP.asset', 'NP V NP P.asset', 'NP V NP PP', 'NP V NP PP PP.instrument', 'NP V NP PP.asset', 'NP V NP PP.attribute', 'NP V NP PP.attribute PP.extent', 'NP V NP PP.beneficiary', 'NP V NP PP.co-agent', 'NP V NP PP.co-patient', 'NP V NP PP.co-theme', 'NP V NP PP.destination', 'NP V NP PP.destination PP.initial_location', 'NP V NP PP.destination-Conative', 'NP V NP PP.destinations', 'NP V NP PP.goal', 'NP V NP PP.initial_location', 'NP V NP PP.initial_location PP.destination', 'NP V NP PP.instrument', 'NP V NP PP.location', 'NP V NP PP.material', 'NP V NP PP.material PP.asset', 'NP V NP PP.material PP.beneficiary', 'NP V NP PP.patient', 'NP V NP PP.predicate', 'NP V NP PP.product PP.beneficiary', 'NP V NP PP.recipient', 'NP V NP PP.recipient PP.asset', 'NP V NP PP.recipient PP.theme', 'NP V NP PP.result', 'NP V NP PP.result PP.instrument', 'NP V NP PP.source', 'NP V NP PP.source NP.asset', 'NP V NP PP.source PP.beneficiary', 'NP V NP PP.source PP.instrument', 'NP V NP PP.source S', 'NP V NP PP.source S_ING', 'NP V NP PP.source whether S', 'NP V NP PP.stimulus', 'NP V NP PP.theme', 'NP V NP PP.theme', 'NP V NP PP.topic', 'NP V NP PP.topic what S', 'NP V NP PP.value', 'NP V NP S', 'NP V NP S-INF', 'NP V NP S-Quote', 'NP V NP S_INF', 'NP V NP S_ING', 'NP V NP that S', 'NP V NP to be ADJ', 'NP V NP to be NP', 'NP V NP together', 'NP V NP up', 'NP V NP what S', 'NP V NP what S_INF', 'NP V NP whether S', 'NP V NP whether S_INF', 'NP V NP-ATTR-POS', 'NP V NP-dative NP', 'NP V NP-Dative NP', 'NP V NP-Fulfilling PP', 'NP V NP-Fulfilling PP.theme', 'NP V NP-PRO-ARB', 'NP V NP.attribute', 'NP V NP.beneficiary NP', 'NP V NP.beneficiary NP PP', 'NP V NP.destination', 'NP V NP.destination PP.theme', 'NP V NP.experiencer', 'NP V NP.initial_location', 'NP V NP.location', 'NP V NP.location PP.theme', 'NP V NP.material', 'NP V NP.material PP.product', 'NP V NP.patient', 'NP V NP.patient PP.material PP.result', 'NP V NP.patient PP.result', 'NP V NP.product PP.material', 'NP V NP.recipient', 'NP V NP.recipient how S_INF', 'NP V NP.recipient S', 'NP V NP.recipient S-Quote', 'NP V NP.recipient S_INF', 'NP V NP.recipient that S', 'NP V NP.recipient when S_INF', 'NP V NP.source', 'NP V NP.stimulus', 'NP V NP.theme', 'NP V NP.theme (PP)', 'NP V NP.theme PP', 'NP V NP.theme PP.destination', 'NP V NP.theme PP.instrument', 'NP V NP.topic', 'NP V NP.value', 'NP V NP:', 'NP V out', 'NP V PP', 'NP V PP ADV-Middle', 'NP V PP PP', 'NP V PP-Conative', 'NP V PP.attribute', 'NP V PP.attribute S_ING', 'NP V PP.beneficiary', 'NP V PP.co-agent', 'NP V PP.co-agent how S', 'NP V PP.co-agent how S_INF', 'NP V PP.co-agent PP.goal', 'NP V PP.co-agent PP.theme', 'NP V PP.co-agent PP.theme S_ING', 'NP V PP.co-agent PP.theme what S', 'NP V PP.co-agent PP.theme what S_INF', 'NP V PP.co-agent PP.theme whether S', 'NP V PP.co-agent PP.theme whether S_INF', 'NP V PP.co-agent PP.topic', 'NP V PP.co-agent PP.topic what S', 'NP V PP.co-agent PP.topic what S_INF', 'NP V PP.co-agent PP.topic whether S_INF', 'NP V PP.co-agent PP.topic whether/if S', 'NP V PP.co-patient', 'NP V PP.co-theme', 'NP V PP.destination', 'NP V PP.destination NP', 'NP V PP.experiencer', 'NP V PP.extent', 'NP V PP.goal', 'NP V PP.goal what S_INF', 'NP V PP.initial_loc', 'NP V PP.initial_loc PP.destination', 'NP V PP.initial_location', 'NP V PP.initial_location PP.destination', 'NP V PP.instrument', 'NP V PP.location', 'NP V PP.location PP.theme', 'NP V PP.material PP.result', 'NP V PP.patient', 'NP V PP.recipient', 'NP V PP.recipient how S', 'NP V PP.recipient how S_INF', 'NP V PP.recipient NP', 'NP V PP.recipient PP.topic', 'NP V PP.recipient PP.topic what S', 'NP V PP.recipient PP.topic what S_INF', 'NP V PP.recipient PP.topic whether S_INF', 'NP V PP.recipient S-Quote', 'NP V PP.recipient S_INF', 'NP V PP.recipient that S', 'NP V PP.recipient what S', 'NP V PP.recipient what S_INF', 'NP V PP.recipient when S_INF', 'NP V PP.recipient whether S', 'NP V PP.recipient whether/if S', 'NP V PP.result', 'NP V PP.result NP', 'NP V PP.result PP.instrument', 'NP V PP.source', 'NP V PP.source how S', 'NP V PP.source how S_INF', 'NP V PP.source PP.goal', 'NP V PP.source PP.theme what S', 'NP V PP.source PP.theme what S_INF', 'NP V PP.source PP.theme whether S_INF', 'NP V PP.source PP.theme whether/if S', 'NP V PP.source S_ING', 'NP V PP.stimulus', 'NP V PP.stimulus how/whether S', 'NP V PP.stimulus S_ING', 'NP V PP.stimulus what S', 'NP V PP.theme', 'NP V PP.theme NP S_ING', 'NP V PP.theme NP.location', 'NP V PP.theme PP.location', 'NP V PP.theme PP.source', 'NP V PP.theme S', 'NP V PP.theme S_INF', 'NP V PP.theme S_ING', 'NP V PP.theme what S', 'NP V PP.theme what S_INF', 'NP V PP.theme whether S_INF', 'NP V PP.theme whether/if S', 'NP V PP.time', 'NP V PP.topic', 'NP V PP.topic NP S_ING', 'NP V PP.topic PP.co-agent', 'NP V PP.topic PP.recipient', 'NP V PP.topic S_ING', 'NP V PP.topic what S', 'NP V PP.topic what S_INF', 'NP V PP.topic whether S', 'NP V PP.topic whether S_INF', 'NP V PP.trajectory', 'NP V PP.value', 'NP V S', 'NP V S-Quote', 'NP V S_INF', 'NP V S_ING', 'NP V that S', 'NP V that S PP.theme', 'NP V together', 'NP V together ADV-Middle', 'NP V up NP', 'NP V what S', 'NP V what S_INF', 'NP V when S_INF', 'NP V whether S', 'NP V whether S_INF', 'NP V whether/if S', 'NP V whether/if S_INF', 'NP V why S', 'NP.agent V', 'NP.agent V NP', 'NP.asset V NP', 'NP.asset V NP NP', 'NP.asset V NP PP', 'NP.asset V PP', 'NP.attribute V', 'NP.attribute V NP.extent', 'NP.attribute V PP.extent', 'NP.cause V NP', 'NP.instrument V ADVP', 'NP.instrument V NP', 'NP.location V', 'NP.location V NP', 'NP.location V NP.theme', 'NP.location V PP.agent', 'NP.location V PP.theme', 'NP.material V NP', 'NP.material V PP.product', 'NP.patient V', 'NP.patient V PP.attribute', 'NP.patient V PP.material PP.result', 'NP.patient V PP.result', 'NP.product V PP.material', 'NP.theme V', 'NP.theme V NP', 'NP.theme V PP.location', 'NP.theme V PP.source', 'Passive', 'PP.location there V NP', 'PP.location V NP', 'PP.location V PP.theme', 'That S.stimulus V', 'There V NP', 'There V NP PP', 'There V NP PP.location', 'There V PP NP' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored intersecting verbs in VerbNet and GLoVe.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def si(dct, key): #safe dictionary indexing procedure\n",
    "    try: return(dct[key]) \n",
    "    except KeyError: return(False)\n",
    "\n",
    "\n",
    "gen_dict = {}\n",
    "for verb, verbnet_vec in verbnet_dict.items():\n",
    "    v = si(wikiglove_dict, verb)\n",
    "    w = si(twitterglove_dict, verb)\n",
    "    if (not v == False and not w == False):\n",
    "        gen_dict[verb] = (verbnet_vec, np.hstack((v,w)))\n",
    "\n",
    "#There are 3988 verbs in the intersection of glove and verbnet\n",
    "print(\"Stored intersecting verbs in VerbNet and GLoVe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good shuffle found!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def shuffle(a, b): \n",
    "    p = np.random.permutation(len(a))\n",
    "    return(a[p], b[p])\n",
    "\n",
    "dat = list(gen_dict.items())\n",
    "l = round(.8 * len(dat))\n",
    "X = np.array([value[1] for _key, value in dat])\n",
    "y_sparse = np.array([value[0] for _key, value in dat])\n",
    "X,y_sparse = shuffle(X,y_sparse)\n",
    "\n",
    "#random.shuffle(y_sparse)\n",
    "\n",
    "sparse_frames = [i for (i,j) in enumerate(np.sum(y_sparse, axis=0)) if j < 100]\n",
    "y = np.delete(y_sparse, sparse_frames,axis=1)\n",
    "rich_frame_list = np.delete(frame_list + [\"\"], sparse_frames, axis=0)\n",
    "\n",
    "m = 0\n",
    "while (m == 0):\n",
    "    X,y = shuffle(X,y)\n",
    "    y_train = y[:l]\n",
    "    y_validation = y[l:]\n",
    "    m = min(min(np.sum(y_train, axis=0)), min(np.sum(y_validation, axis=0)))\n",
    "\n",
    "X_train = np.array(X[:l])\n",
    "X_validation = np.array(X[l:])\n",
    "\n",
    "print(\"Good shuffle found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determing list of diagnostic frames, \n",
    "\n",
    "def jd(i,j): #Jaccard metric distance between two frames\n",
    "    ci = 0\n",
    "    cj = 0\n",
    "    cij = 0\n",
    "    for v in y:\n",
    "        ci += v[i]\n",
    "        cj += v[j]\n",
    "        cij += (v[i] + v[j])//2\n",
    "    if ci == 0 and cj == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(1 - (cij)/(ci + cj - cij))\n",
    "        \n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "n_frames = y.shape[1]\n",
    "dist_matrix = np.zeros((n_frames, n_frames))\n",
    "for i in range(0, n_frames):\n",
    "    for j in range(i, n_frames):\n",
    "        d = jd(i,j)\n",
    "        dist_matrix[i][j] = d\n",
    "        dist_matrix[j][i] = d\n",
    "        \n",
    "aff = np.exp(- dist_matrix ** 2 / (2. * 1 ** 2)) #Gaussian heat kernel\n",
    "\n",
    "n_diagnostic_frames = 50\n",
    "\n",
    "sc_frames = SpectralClustering(n_clusters = n_diagnostic_frames, affinity = \"precomputed\")\n",
    "frame_cluster_predictions = sc_frames.fit_predict(aff)\n",
    "frame_cluster_pairs = list(enumerate(frame_cluster_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "glove_verbs = list(map(lambda v : (v[0]), list(gen_dict.items())))\n",
    "glove_vecs = np.array(list(map(lambda v : (v[1][1]), list(gen_dict.items()))))\n",
    "\n",
    "n_diagnostic_verbs = 100\n",
    "\n",
    "kmc_verbs = KMeans(n_clusters = n_diagnostic_verbs)\n",
    "verb_cluster_predictions = kmc_verbs.fit_predict(glove_vecs)\n",
    "\n",
    "verb_cluster_pairs = list(enumerate(verb_cluster_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity =  0.0976\n",
      "Size = (50, 25)\n"
     ]
    }
   ],
   "source": [
    "def cluster_sample(l, n_clusters):\n",
    "    res = []\n",
    "    random.shuffle(l)\n",
    "    for i in range(0, n_clusters):\n",
    "        for v,c in l:\n",
    "            if i == c:\n",
    "                bv = v\n",
    "        res += [bv]\n",
    "    return(res)\n",
    "        \n",
    "import pandas as pd\n",
    "\n",
    "def maketable():\n",
    "    diagnostic_verb_list = [glove_verbs[i] for i in cluster_sample(verb_cluster_pairs, n_diagnostic_verbs)]\n",
    "    diagnostic_frame_list = [frame_list[i] for i in cluster_sample(frame_cluster_pairs, n_diagnostic_frames)]\n",
    "\n",
    "    if len(diagnostic_verb_list) - len(list(set(diagnostic_verb_list))) > 0:\n",
    "        return(maketable())\n",
    "    elif len(diagnostic_frame_list) - len(list(set(diagnostic_frame_list))) > 0:\n",
    "        return(maketable())\n",
    "    else:\n",
    "        data = np.zeros((n_diagnostic_verbs, n_diagnostic_frames))\n",
    "        for i in range(0, n_diagnostic_verbs):\n",
    "            for j in range(0, n_diagnostic_frames):\n",
    "                data[i][j] = verbnet_dict[diagnostic_verb_list[i]][ frame_list.index(diagnostic_frame_list[j]) ]\n",
    "\n",
    "        df = pd.DataFrame(data, index = diagnostic_verb_list, columns = diagnostic_frame_list, dtype=int)\n",
    "\n",
    "\n",
    "        for i in range(0, n_diagnostic_verbs//2):\n",
    "            verb = pd.Series.argmin(df.sum(axis=1))\n",
    "            df = df.drop(verb, axis=0)\n",
    "\n",
    "        for i in range(0, n_diagnostic_frames//2):\n",
    "            frame = pd.Series.argmin(df.sum(axis=0))\n",
    "            df = df.drop(frame, axis=1)\n",
    "\n",
    "        return(df)\n",
    "\n",
    "btable = maketable()\n",
    "bcount = pd.DataFrame.sum(df.sum(axis=0))\n",
    "for i in range(0, 100):\n",
    "    t = maketable()\n",
    "    count = pd.DataFrame.sum(t.sum(axis=0))\n",
    "    if count > bcount:\n",
    "        btable = t\n",
    "        bcount = count\n",
    "df = btable\n",
    "\n",
    "print(\"Sparsity = \", pd.DataFrame.sum(df.sum(axis=0)) / (n_diagnostic_frames//2 * n_diagnostic_verbs//2))\n",
    "print(\"Size =\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr = OneVsRestClassifier(LogisticRegression())\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811214786228\n"
     ]
    }
   ],
   "source": [
    "def avg_auc(pred):\n",
    "    ras = []\n",
    "    for i in range(0, y_validation.shape[1]):\n",
    "        y_true = y_validation[:,i]\n",
    "        y_pred = pred[:,i]\n",
    "        ras += [roc_auc_score(y_true, y_pred)]\n",
    "    return(np.mean(ras))\n",
    "\n",
    "def auc_scatter(pred):\n",
    "    aucs = []\n",
    "    texps = []\n",
    "    for i in range(0, y_validation.shape[1]):\n",
    "        texps += [np.sum(y_validation, axis=0)[i]]\n",
    "        aucs += [roc_auc_score(y_validation[:,i], pred[:,i])]\n",
    "    plt.clf()\n",
    "    plt.xlabel(\"Number of positive training examples\")\n",
    "    plt.ylabel(\"AUROC value\")\n",
    "    plt.scatter(texps, aucs, s=5)\n",
    "            \n",
    "def auc_plot(pred):\n",
    "    plt.clf()\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    ras = []\n",
    "    for i in range(0, y_validation.shape[1]):\n",
    "        y_true = y_validation[:,i]\n",
    "        y_score = pred[:,i]\n",
    "        fpr = roc_curve(y_true, y_score)[0]\n",
    "        tpr = roc_curve(y_true, y_score)[1]\n",
    "        plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1])\n",
    "    \n",
    "probs = lr.predict_proba(X_validation)\n",
    "#probs = BPMLL_predict_proba\n",
    "\n",
    "print(avg_auc())\n",
    "auc_plot(lr.predict_proba(X_validation))\n",
    "plt.savefig(\"auc_plot100.png\")\n",
    "\n",
    "#auc_scatter(lr.predict_proba(X_validation))\n",
    "#plt.savefig(\"auc_scatter5rand.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NP V' 'NP V ADV-Middle' 'NP V how S' 'NP V how S_INF' 'NP V NP'\n",
      " 'NP V NP ADJ' 'NP V NP ADJP' 'NP V NP PP.attribute'\n",
      " 'NP V NP PP.beneficiary' 'NP V NP PP.co-patient' 'NP V NP PP.destination'\n",
      " 'NP V NP PP.instrument' 'NP V NP PP.location' 'NP V NP PP.recipient'\n",
      " 'NP V NP PP.result' 'NP V NP PP.source' 'NP V NP PP.theme' 'NP V NP S_ING'\n",
      " 'NP V NP together' 'NP V NP-PRO-ARB' 'NP V NP.destination'\n",
      " 'NP V NP.patient' 'NP V NP.theme' 'NP V PP' 'NP V PP.location'\n",
      " 'NP V PP.recipient' 'NP V PP.recipient S-Quote' 'NP V PP.stimulus'\n",
      " 'NP V PP.theme' 'NP V PP.topic' 'NP V S_INF' 'NP V S_ING' 'NP V that S'\n",
      " 'NP V what S' 'NP.cause V NP' 'NP.instrument V NP'\n",
      " 'NP.location V PP.theme' 'NP.patient V' 'PP.location V NP' 'There V NP PP'\n",
      " 'There V PP NP']\n"
     ]
    }
   ],
   "source": [
    "print(rich_frame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating MatLab-compatible data\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "train_data = X_train #(X_train - X_train.min(0)) / X_train.ptp(0)\n",
    "train_target = (2 * y_train - 1).T\n",
    "\n",
    "test_data = X_validation #(X_validation - X_validation.min(0)) / X_validation.ptp(0)\n",
    "test_target = (2 * y_validation - 1).T\n",
    "\n",
    "scipy.io.savemat('train_data.mat', mdict={'train_data': train_data})\n",
    "scipy.io.savemat('train_target.mat', mdict={'train_target': train_target})\n",
    "scipy.io.savemat('test_data.mat', mdict={'test_data': test_data})\n",
    "scipy.io.savemat('test_target.mat', mdict={'test_target': test_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPMLL_predict_proba = .5 * (scipy.io.loadmat(\"Outputs.mat\")[\"Outputs\"].T + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "clf_mlp = MLPClassifier(activation = 'relu', \n",
    "                        batch_size = 200, \n",
    "                        solver='adam', \n",
    "                        alpha=1e-4,\n",
    "                        hidden_layer_sizes=(8000,), \n",
    "                        random_state=8, \n",
    "                        early_stopping=True, \n",
    "                        validation_fraction = .15)\n",
    "clf_mlp.fit(X_train, y_train)\n",
    "print(auc(clf_mlp.predict_proba(X_validation)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def sloppy_predict(predict_proba, n):\n",
    "    '''\n",
    "    predictions:\n",
    "        predictions = BPMLL_predict_proba\n",
    "            for matlab model\n",
    "        predictions = model.predict_proba(X)\n",
    "            for sklearn model\n",
    "        predictions = np.array([1 - X.T[0] for X in nn.predict_proba(X_validation)]).T \n",
    "            for sknn models\n",
    "    '''\n",
    "    predictions = copy.copy(predict_proba)\n",
    "    for vec in predictions:\n",
    "        ceil = np.sort(vec)[::-1][n]\n",
    "        for i in range(0, len(vec)):\n",
    "            if vec[i] > ceil:\n",
    "                vec[i] = 1\n",
    "            else:\n",
    "                vec[i] = 0\n",
    "    return(predictions)\n",
    "\n",
    "def alpha_predict(predict_probas, alpha):\n",
    "    predictions = copy.copy(predict_probas)\n",
    "    for vec in predictions:\n",
    "        vec_sorted = np.sort(vec) #smallest to largest\n",
    "        vec_transformed = (1 - vec_sorted)\n",
    "        mi = 0\n",
    "        for index,proba in enumerate(np.cumprod(vec_transformed)):\n",
    "            if proba > (1 - alpha):\n",
    "                mi = index\n",
    "        ceil = vec_sorted[mi]\n",
    "        #print(ceil)\n",
    "        for i in range(0, len(vec)):\n",
    "            if vec[i] > ceil:\n",
    "                vec[i] = 1\n",
    "            else:\n",
    "                vec[i] = 0\n",
    "                \n",
    "    return(predictions)\n",
    "        \n",
    "def count_unpredicted(X_predicted, X_actual):\n",
    "    arr = X_actual - X_predicted\n",
    "    total = 0\n",
    "    for i in arr.flatten():\n",
    "        if i == 1:\n",
    "            total += 1\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737, 292)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_pred(p):\n",
    "    return(np.tile(np.array(list(map(lambda x : int(x > p), (np.sum(y_train, axis=0)/len(y_train))))), (len(y_validation),1)))\n",
    "\n",
    "naive_pred(.5).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
